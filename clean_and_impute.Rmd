---
title: "sp_clean_impute"
author: "Bianca Brusco bb1569"
date: "10/18/2018"
output: html_document
---

    ```{r setup, include = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE)
    library(lattice)
    library(spdep)
    library(RColorBrewer)
    library(classInt)
    library(rgdal)
    library(formatR)
    library(sm)
    library(tidyverse)
    #library(revgeo)
    library(ggmap)
    library(sp)
    library(reshape2)
    library(lubridate)
    library(MASS)
    library(gstat) 
    library(sp)
    library(gganimate)
    library(rgeos)
    library(maptools)
    library(transformr)
    library(rmapshaper)
    library(animation)
    library(magick)
    library(GISTools)
    
    #snippet to override r-chunk fonts.
    def.chunk.hook  <- knitr::knit_hooks$get("chunk")
    knitr::knit_hooks$set(chunk = function(x, options) {
       x <- def.chunk.hook(x, options)
       ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
    })
    ```

## Introduction

Stop and Frisk in New York City was a program of the NYPD (New York Police Department) to control crime, which involved stopping, questioning and in some cases searching, citizens on the streets. The program was highly controversial, due to the high discretionary power given to the police in choosing who to stop and where. The data on the number and type of stops, which is now publicly available, has been used to understand whether the stops had an effect in reducing crimes, whether they were racially charged, or a number of other questions on the program. 

In this project, we are interested in learning more about how wealth of a specific neighbourhood related, during these years, to the number of stops performed by the police, and whether the first is a good predictor of the second. We are curious in this question because it gives a starting point for investigation on whether the police discriminated neighborhoods in terms of their wealth. While true that poorer neighborhoods have a higher crime rate, it is also true that in NYC there is a large amount of commuting and flux of people that spends their day in different areas to where they usually live. Our hypothesis, therefore, is that if there is no discrimination in terms of neighbourhood wealth, the housing value at a specific zip code will only be weakly correlated with the number of stops performed there. The opposite will occur if poorer neighborhoods were targetted more heavily than needed, although the relationship would be then further examined to establish causality. 

As a metric for 'neighbourhood wealth' we have decided to use housing value, which is representative of the residential value of a neighbourhood, rather than of the people who are in it during the day.  We look at yearly housing value by zip code, and number of stops and frisk rate over the zip code area, to understand how the relationship among these variable has changed. Specifically, we are interested in exploring areas that have seen a large change in housing values over the 7 years of the program, to see whether the number of frisks in this area have changed as well. 

In the project, we start by exploring the data available to us, to identify patterns. We then attempt at fitting a model that takes in considerations the variables of interest, taking in consideration both the spatial and the temporal components of the problem at hand. 


#1. Importing and Cleaning data

In this project we used two sources of data. The first one is the data from NYC Open Data of Stops and Frisks throughout the years of 2006 and 2013. For the purpose of our project we aggregated this data at the zip-code and yearly level. The second source of data is the housing value index made available by the company Zillow at https://www.zillow.com/research/data/. We downloaded Zillow Housing Value Index Data at the grain of zip-code and month, for the years of interest. 

##A. Importing data

(i). Housing value panel data

```{r init-1,size="tiny",warning=FALSE,message=FALSE}
# Housing value
hvraw <- read.csv("data/Trimmed_home_value.csv", header = T)
```

(ii). NYC Stop & Frisk data

```{r}
# Stop & Frisk
load("data/sqf.RData")
```

##B. Cleaning data

(i) Housing Value

We first trim the ZHVI dataset to only contain the years we are interest in (2016 until 2013), as well as only our city of interest, New York City. 

```{r}
# select relevant date range for housing value, filter out other cities
hv <- hvraw %>% filter(City == "New York") %>% dplyr::select(RegionName, CountyName, X2005.12:X2014.01) %>% arrange(RegionName)
```

(ii) Stop & Frisk

We then move to cleaning the data's spatial attributes. We note that in the Stop&Frisk dataset there are some latitudes and longitudes that appear to be a mistake, as they show outside the boundaries of the city. To delete these observations, we look at the geographical boundaries of NYC, and then removed all the observations that laid outside of it. Source of geographic boundaries of NYC: https://www.mapdevelopers.com/geocode_bounding_box.php  
  
North Latitude: 40.917577 South Latitude: 40.477399 East Longitude: -73.700272 West Longitude: -74.259090


```{r}
# filter out NAs for lon and lat, 1303 NAs and 47 "1900-12-31" for date, then select columns of interest, sorted in chronological order
sf <- stops %>% filter(lat > 40.2 & !is.na(lat) & lat < 41) %>% filter(date >= "2006-01-01" & date <= "2013-12-31") %>% dplyr::select(year, date, time, precinct, suspected.crime, frisked, suspect.race, found.weapon, lat, lon) %>% arrange(date, time) %>% mutate(month = substr(date, 6, 7))

```


#### shapefiles for zip code bounds and NYC

In order to merge the two data files, we used the Zip Code shapefile, avilable of NYC Open Data (https://data.cityofnewyork.us/Business/Zip-Code-Boundaries/i8iw-xf4u). The shapefile allowed us to identify within which Zip code a stop happened. 

```{r, echo = F}
# zip code shape file
zipraw <- readOGR("data/zip_code/ZIP_CODE_040114.shp")
nyc <- readOGR("data/Borough_Boundaries/geo_export_5e515234-1937-40b5-b942-1ef10ea3ea45.shp")

# change projection method to lon/lat
zipbd <-  spTransform(zipraw, CRS("+proj=longlat +ellps=WGS84 +no_defs"))
```

#### Add zip code field to Stop & Frisk based on coordinates

We check over which zip-code polygon each of the stops is. We then examine which stops have a missing values with the below plot:

```{r}
coord1 <- cbind(sf$lon, sf$lat)
points <- SpatialPoints(coord1, CRS("+proj=longlat +ellps=WGS84 +no_defs"))
reversezip <- over(points, zipbd)

plot(zipbd, border = "black", lwd = 0.2)
points(sf$lon[which(is.na(reversezip$ZIPCODE))], sf$lat[which(is.na(reversezip$ZIPCODE))], pch = ".", cex = 2.5, col = "red", main = "Missing 'matches'")
```


It appears that all the missing values lie right on the boundaries of two different zipcodes, which is why the overlaying function does not work appropriately. To fix this, we will find 8 surrounding points (all of whom of equal distance to the original point), from north going clockwise. We will then get the zipcode for all those 8 points and exclude NAs before taking a majority vote. Finally, we use the majority vote result to impute the NAs.

**function for matching boundary points**

```{r}
# The distance from the original point is about 370 ft.
zipcorrect <- function(lon, lat, shp) {
  revzip <- rep(NA, length(lon))
  for (i in 1:length(lon)) {
    # starting from north, rotating clockwise for the 8 sample points
    samp.lon <- c(lon[i], lon[i] + 0.001, lon[i] + 0.0014, lon[i] + 0.001, lon[i], lon[i] - 0.001, lon[i] - 0.0014, lon[i] - 0.001)
    samp.lat <- c(lat[i] + 0.0014, lat[i] + 0.001, lat[i], lat[i] - 0.001, lat[i] - 0.0014, lat[i] - 0.001, lat[i], lat[i] + 0.001)
    
    samp.coord <- cbind(samp.lon, samp.lat)
    
    samp.points <- SpatialPoints(samp.coord, CRS("+proj=longlat +ellps=WGS84 +no_defs"))
    
    samp.zip <- as.character(over(samp.points, shp)$ZIPCODE)
    shortzip <- samp.zip[which(!is.na(samp.zip))]
    
    revzip[i] <- shortzip[which.max(tabulate(match(shortzip, unique(shortzip))))]
  }
  return(revzip)
}
```

And we join the boundary points to the Zipcode

```{r}
zipmiss <- sf[which(is.na(reversezip$ZIPCODE)), c(10, 9)]
imputedzip <- zipcorrect(zipmiss$lon, zipmiss$lat, zipbd)
```

**Append zipcode and zipcode related fields to Stop & Frisk**
  
```{r}
sf$zip <- as.character(reversezip$ZIPCODE)
sf$zip[which(is.na(sf$zip))] <- imputedzip
```

##C. group stop and frisk data based zipcode

We now add attributes from the ZIPCODE data table, such as population and area. 

```{r}
jointable <- zipbd@data %>% dplyr::select(zip = ZIPCODE, po_name = PO_NAME, pop = POPULATION, area = AREA, county = COUNTY) %>% arrange(zip, desc(area))
```

Within the jointable, there is one duplicate record of zipcode 10047. There are other regions with multiple polygons associated with one zip code (10004 for instance). For these cases, we aggregate the land area and take any one of the population entry. It is cleaned in the following chunk.

```{r}
jtclean <- jointable[!duplicated(jointable), ]
jtclean <- jtclean %>% group_by(zip) %>% summarize(po_name = first(po_name), pop = first(pop), area = sum(area), county = first(county)) %>% as.data.frame()

jtclean$zip <- as.character(jtclean$zip)
jtclean$county <- as.character(jtclean$county)

jtclean$county[which(jtclean$county == "New York")] <- "Manhattan"
jtclean$county[which(jtclean$county == "Richmond")] <- "Staten Island"
jtclean$county[which(jtclean$county == "Kings")] <- "Brooklyn"
colnames(jtclean)[5] <- "borough"
```


```{r}

## merge the stop and frisk file with the clean join table, as well as the zip file
sf_large <- left_join(sf, jtclean)
zip_large <- right_join(sf, jtclean)
```

## Group data on zipcode
```{r}

sf_rate <- sf_large %>% group_by(zip, year, month) %>% summarise(frisked = sum(frisked), stops = n(), po_name = first(po_name), pop = first(pop), area = first(area), borough = first(borough)) %>% as.data.frame()


```

## inspect difference between zipbd, housing value, and stop and frisk

To understand why there is more zipcodes in the shape file than in the Stop and Frisk files, we analyze the zipcodes dataset:
```{r}
kable(head(zipbd@data,3))
```

We note that there is a column called BLGZIP, which is a binary variable that indicates whether a particular zipcode is specific to a certain building. For exmample, the World Trade Center has its own Zip Code, as well as other important buildings in the city. Since these zipcodes are private or enclosed spaces, there are no stops within them. We remove any zipcodes for which BLDGZIP = TRUE. 

```{r}
nbzipbd <- subset(zipbd, BLDGZIP == 0)
# now stop and frisk matches the shapefile zips. 
```

After subsetting this way, there are the same number of zipcodes in the shape file as there are in the Stop and Frisk file. 

We now want to joing the ZHVI (Zillow Housing Value Index) data with the Stop and Frisk file. To do so, we need to put the ZHVI data in a long format. For each zip, month, year, we want to have a specific ZHVI so we can join with the rates table.

```{r}
hvlong = melt(hv, id.vars = c("RegionName", "CountyName"), value.name = "zhvi")
hvlong = hvlong[, - 2]  #don't need county name, drop it
# cleaning because the data variable is in weird format (i.e. X2005
hvlong = hvlong %>% mutate(variable = gsub("X","",variable)) %>% mutate(variable = gsub("\\.","-",variable)) %>% arrange(RegionName) 
hvlong = hvlong %>%  mutate(variable = as.Date(paste0(variable, "-01"))) %>% mutate(year = lubridate::year(variable), month = lubridate::month(variable)) %>% rename(zip = RegionName)

hvlong = hvlong[,c(1,4,5,3)] #reorder 
hvlong$zip = as.character(hvlong$zip)
sf_rate$year = as.double(sf_rate$year) #change variable type for join to work
sf_rate$month = as.double(sf_rate$month)

sfhv = left_join(sf_rate, hvlong, by = c("zip","year", "month"))
head(sfhv,3)

```

```{r}
paste("number of missing values for ZHVI:",length(sfhv[is.na(sfhv$zhvi),]))
```



We note that there is some missing values for ZHVI. Looking at the Zillow Research webiste we find that there is different types of ZHVI -- some are an average of all the housing prices and the other for specific housing type (i.e. condominums). For some zipcodes only one specific housing type is available -- specifically it seems that some zipcodes only have condominiums. Therefore, we download the Condominum&Appts ZHVI to augment the dataset above, and use that value for some of the missing ZHVI. The data can still be found at https://www.zillow.com/research/data/.

```{r}
hvcraw <- read.csv("data/Zip_Zhvi_Condominum.csv", header = T)

# select relevant date range for housing value, filter out other cities
hvc <- hvcraw %>% dplyr::filter(City == "New York") %>% dplyr::select(RegionName, CountyName, X2005.12:X2014.01) %>% dplyr::arrange(RegionName)


## same cleaning as before.......
hvclong = melt(hvc, id.vars = c("RegionName", "CountyName"), value.name = "zhvi")
hvclong = hvclong[, - 2]  #don't need county name, drop is as it is annoying me


hvclong = hvclong %>% mutate(variable = gsub("X","",variable)) %>% mutate(variable = gsub("\\.","-",variable)) %>% arrange(RegionName)
hvclong = hvclong %>%  mutate(variable = as.Date(paste0(variable, "-01"))) %>% mutate(year = lubridate::year(variable), month = lubridate::month(variable)) %>% rename(zip = RegionName, zhvic = zhvi)

hvclong = hvclong[,c(1,4,5,3)]
hvclong$zip = as.character(hvclong$zip)

hvlong_year = hvlong %>% group_by(zip, year) %>% summarise(zhvi = mean(zhvi))
hvclong_year = hvclong %>% group_by(zip, year) %>% summarise(zhvic = mean(zhvic))

hvlong_year = full_join(hvlong_year, hvclong_year, by = c("zip", "year"))
hvlong_year[is.na(hvlong_year$zhvi),]$zhvi = hvlong_year[is.na(hvlong_year$zhvi),]$zhvic
hvlong_year = hvlong_year[,-4]
```

Summarize SF by year:

```{r}
sf_year_full <- sf_rate %>% group_by(zip, year) %>% summarise(frisked = sum(frisked), stops = sum(stops), po_name = first(po_name), pop = mean(pop), area = first(area), borough = first(borough)) %>% mutate(frate = frisked/stops)%>% ungroup() %>% complete(nesting(zip, po_name, pop, area, borough),year, fill = list(frisked = 0, stops = 0, frate = 0))  %>%  as.data.frame()


# Remove the ones with no population
sf_year = sf_year_full %>% filter(pop > 0)
```

Join by year:

```{r}
sfhv_full = full_join(sf_year, hvlong_year, by = c("zip","year"))

# Plot to see missing values

plot(nbzipbd, border = "white", lwd = 0.2, col = ifelse(as.character(zipbd@data$ZIPCODE) %in% unique(sfhv[is.na(sfhv$zhvi),]$zip), "indianred", "steelblue"), main = "ZIP Codes with NA ZHVI, to impute")

```

We note that there are several missing values that are localized in the Bronx and in Upper Manhattan. It looks like Zillow does not have data for these zipcodes. We looked on the website if we could find non-compiled housing value index information for some of these zipcodes, but with no luck. Since these areas are all clustered together and therefore have (we assume) strong spatial correlation with each other, we are wary of imputing them from neighbouring values or kringing.

Note: a possible solution for future investigation is to use actual housing sales data which is made available at: https://www1.nyc.gov/site/finance/taxes/property-rolling-sales-data.page . However, this data is unaggregated data for each neighbourhood, which in practical terms meant to clean and aggreagate 40 data files. In the interest of time and brevity for the project, we decided against this solution. 

Remove Bronx, Upper Manhattan (Inwwod and Washington heights) and Central Park

```{r}
sfhv = sfhv_full %>% filter(borough != "Bronx", po_name != "Central Park", !(zip %in% c("10031", "10032", "10033", "10034", "10040")))
```

Checking:

```{r echo=FALSE}

paste("Number of missing zhvi: ", length(sfhv[is.na(sfhv$zhvi),]$zhvi))
paste("Unique zips in original SF:", length(unique(sf_rate$zip)))
#After removing Bronx and Central Park
paste("Unique numer of zip: ", length(unique(sfhv$zip)))
paste("Number of unique missing zhvi: ", length(unique(sfhv[is.na(sfhv$zhvi),]$zip)))

```

```{r}
plot(nbzipbd, border = "white", lwd = 0.2, col = ifelse(as.character(zipbd@data$ZIPCODE) %in% unique(sfhv[is.na(sfhv$zhvi),]$zip), "indianred", "steelblue"), main = "ZIP Codes with NA ZHVI, to impute")
```


*****STOPPED HERE! THIS HAS NOT NO INWOOD< NO CENTRAL PARK< NOW WASHINGTON HEIGHTS<NO BRONX!!!!*****

## Areal Interpolation

```{r}
#subset the zipcodes file
subzipbd = subset(nbzipbd, ZIPCODE %in% unique(sfhv$zip))

# Let's start by doing it for one year
sfhv_2013 = sfhv %>% filter(year == 2013)
plot(subzipbd, border = "white", lwd = 0.2, col = ifelse(as.character(subzipbd@data$ZIPCODE) %in% unique(sfhv_2013[is.na(sfhv_2013$zhvi),]$zip), "indianred", "steelblue"), main = "ZIP Codes with NA ZHVI, to impute")
```

# merging?

```{r}
subzipbd@data = left_join(subzipbd@data[,c(1,2)], sfhv_2013, by = c("ZIPCODE" = "zip"))
sfhv_df1 = left_join(distinct(subzipbd@data[,c(1,2)]), sfhv, by = c("ZIPCODE" = "zip"))
```


## Exploratory Data Analysis


```{r}



zipbd_df <- fortify(nbzipbd, region = "ZIPCODE")
zipbd_df <- merge(zipbd_df, sf_year_full, by.x = "id", by.y = "zip")

#p <- ggplot() + geom_polygon(data = zipbd_df, aes(long, lat, group = group, fill = frisked), color = alpha("black", 0.5), size = 0.2) + scale_colour_gradient(aes(frame = year), low = "white", high = "red") + theme(legend.position = "none") + coord_map("mercator")

sf_cut <- zipbd_df %>% dplyr::select(id, year, frisked, stops, frate) %>% distinct() %>% group_by(year) %>% mutate(stopxtrm = stops > quantile(stops, seq(0, 1, 0.05))[20], ratextrm = frate > quantile(frate, seq(0, 1, 0.05))[20]) %>% ungroup()

# centroid for labels
cntr_df <- data.frame(unname(cbind(as.character(nbzipbd@data$ZIPCODE), gCentroid(nbzipbd, byid = TRUE)@coords)))
colnames(cntr_df) <- c("zip", "lon", "lat")

cntr_df$zip <- as.character(cntr_df$zip)
cntr_df$lon <- as.numeric(as.character(cntr_df$lon))
cntr_df$lat <- as.numeric(as.character(cntr_df$lat))

zipcntr_df <- cntr_df %>% group_by(zip) %>% summarise(lon = mean(lon), lat = mean(lat)) %>% expand(nesting(zip, lon, lat), year = 2006:2013)

labels_df <- left_join(zipcntr_df, sf_cut, by = c("zip" = "id", "year" = "year"))


theme_opts <- list(theme(panel.grid.minor = element_blank(),
                         panel.grid.major = element_blank(),
                         panel.background = element_blank(),
                         plot.background = element_blank(),
                         panel.border = element_blank(),
                         axis.line = element_blank(),
                         axis.text.x = element_blank(),
                         axis.text.y = element_blank(),
                         axis.ticks = element_blank(),
                         axis.title.x = element_blank(),
                         axis.title.y = element_blank(),
                         legend.position="right",
                         plot.title = element_text(size = 20)))

#-74.15, 40.8

zipbd_df %>% filter(year == 2006) %>% ggplot(aes(long, lat, group = group, fill = stops), color = alpha("white", 0.5), size = 0.2) + theme_opts + geom_polygon() + scale_fill_viridis_c(begin = 0.9, end = 0.1, direction = 1, alpha = 0.8) + coord_map("mercator") + geom_text(data = labels_df %>% filter(year == 2006), aes(lon, lat, group = zip, label = ifelse(stopxtrm, zip, '')), size = 3) + annotate("text", size = 10, x = -74.15, y = 40.8, label = year[1])


shnomiss %>% arrange(shnomiss@data, shnomiss@data$zhvi)
shnomiss@data[which(max(shnomiss@data$zhvi)),]

mod1 = lm(stops ~ zhvi_z, data = nomanh@data)
summary(mod1)

datalist <- split(zipbd_df, zipbd_df$year)

img1 <- image_graph(1000, 750, res = 96)
out1 <- lapply(datalist, function(dat){
  p <- dat %>% ggplot(aes(long, lat, group = group, fill = sqrt(stops)), color = alpha("white", 0.5), size = 0.2) + theme_opts + geom_polygon() + scale_fill_viridis_c(begin = 1, end = 0.4, direction = 1, alpha = 0.8, guide = guide_colorbar(label = F, ticks = F, title = "Stops")) + coord_map("mercator")
  
  p <- p + geom_text(data = labels_df %>% filter(year == dat$year[1]), aes(lon, lat, group = zip, label = ifelse(stopxtrm, zip, '')), size = 2.5)
    
  print(p + annotate("text", size = 10, x = -74.15, y = 40.8, label = dat$year[1]))
})
dev.off()
animation1 <- image_animate(img1, fps = 1)
print(animation1)

image_write(animation1, "stop2.gif")

img2 <- image_graph(1000, 750, res = 96)
out2 <- lapply(datalist, function(dat){
  p <- dat %>% ggplot(aes(long, lat, group = group, fill = frate), color = alpha("white", 0.5), size = 0.2) + theme_opts + geom_polygon() + scale_fill_distiller(palette = "YlOrRd", direction = 1, guide = guide_colorbar(label = F, ticks = F, title = "Frisk Rate")) + coord_map("mercator")
  
  p <- p + geom_text(data = labels_df %>% filter(year == dat$year[1]), aes(lon, lat, group = zip, label = ifelse(ratextrm, zip, '')), col = "black", size = 3)
    
  print(p + annotate("text", size = 10, x = -74.15, y = 40.8, label = dat$year[1]))
})
dev.off()
animation2 <- image_animate(img2, fps = 1)
print(animation2)

image_write(animation2, "frate1.gif")
```

```{r}
####
p <- ggplot(zipbd_df, aes(long, lat, group = group, fill = frisked), color = alpha("white", 0.5), size = 0.2) + theme_opts + geom_polygon() + scale_fill_distiller(palette = "Greens", direction = 1) + coord_map("mercator")

ggplot(data, aes(long, lat, group = group, fill = frisked), color = alpha("white", 0.5), size = 0.2) + theme_opts + geom_polygon() + scale_fill_distiller(direction = 1) + coord_map("mercator") + labs(title = data$year)

ggplot(data, aes(long, lat, group = group, fill = frisked, color = alpha("white", 0.5), size = 0.2)) + theme_opts + geom_polygon() + scale_fill_manual(palette = "Greens") + coord_map("mercator") + labs(title = data$year)
####
#full_sfhv
sfhv_df1$zhvi_z = as.numeric(scale(sfhv_df1$zhvi))
sfhv_df1$zhvi_invlog = log(1/(abs(sfhv_df1$zhvi_z)))
plotdf <- sfhv_df1[which(!is.na(sfhv_df1$zhvi)), ]

theme_opts_scatter <- list(theme(panel.grid.minor = element_blank(),
                                 plot.background = element_blank()))

ggplot(plotdf, aes(x = stops, y = zhvi_z, color = frate)) +
  geom_point(alpha = 0.7, size = 6) +
  scale_color_distiller(palette = "YlOrRd", direction = 1, guide = guide_colorbar(label = F, ticks = F, title = "Frisk Rate")) +
  #scale_size(range = c(2, 10)) +
  scale_x_sqrt() +
  facet_wrap(~ borough) +
  theme_opts_scatter +
  labs(title = 'Year: {floor(frame_time)}', x = 'Logarithm of Stops', y = 'Housing Value') +
  transition_time(year) +
  ease_aes('linear')


#anim_save()
```



```{r}
choropleth(subzipbd, subzipbd@data$stops)

NYC.nb <- poly2nb(subzipbd)
NYC.lw <- nb2listw(NYC.nb, zero.policy =  TRUE)
geary.mc(subzipbd@data$stops, NYC.lw, nsim = 999, zero.policy = TRUE)
```


```{r}
library(sp)
library(rgdal)
library(raster)
library(gstat)


misszips = unique(subzipbd@data[is.na(subzipbd@data$zhvi),]$ZIPCODE)
shmiss = subset(subzipbd, ZIPCODE %in% misszips )

shnomiss = subset(subzipbd, !(ZIPCODE %in% misszips))


r <- raster(subzipbd)
g <- as(r, 'SpatialGrid')


vg = variogram( zhvi ~ 1, data = shnomiss)

fit.wls = fit.variogram(vg, vgm(c("Lin", "Exp", "Sph", "Gau")))
print(fit.wls)
plot(vg)
plot(vg, fit.wls,
main = paste0("Best model = ", as.character(fit.wls$model[2])))

misszips = unique(subzipbd@data[is.na(subzipbd@data$zhvi),]$ZIPCODE)
shmiss = subset(subzipbd, ZIPCODE %in% misszips )
shnomiss = subset(subzipbd, !(ZIPCODE %in% misszips))
r <- raster(subzipbd)
g <- as(r, 'SpatialGrid')
vg = variogram( zhvi ~ 1, data = shnomiss)
fit.wls = fit.variogram(vg, vgm(c("Lin", "Exp", "Sph", "Gau")))
print(fit.wls)
plot(vg)
plot(vg, fit.wls,
main = paste0("Best model = ", as.character(fit.wls$model[2])))
```

```{r}
k <- gstat(formula=shnomiss$zhvi~1, locations=shnomiss, model=fit.wls)
kp <- predict(k, g)
spplot(kp)
```


## Modeling

Starting For 2013:


```{r}
#For 2013 data
subzipbd@data$zhvi_z = scale(subzipbd@data$zhvi)
subzipbd@data$zhvi_invlog = log(1/(abs(subzipbd@data$zhvi_z)))
nomanh = subset(subzipbd, borough != "Manhattan" )
plot(nomanh@data$frate, nomanh@data$zhvi_z,  pch = 16, ylab = "Z scores for ZHVI", xlab = "frisk rate", col = factor(nomanh@data$borough))
legend("topright", legend = unique(factor(nomanh@data$borough)), col = unique(factor(nomanh@data$borough)), pch = 19)


mod1 = lm(stops ~ zhvi_z, data = nomanh@data)
summary(mod1)
```



```{r}
k <- gstat(formula=shnomiss$zhvi~1, locations=shnomiss, model=fit.wls)
kp <- predict(k, g)
spplot(kp)
```





## Modeling

Starting For 2013:


```{r}
#For 2013 data
subzipbd@data$zhvi_z = scale(subzipbd@data$zhvi)
subzipbd@data$zhvi_invlog = log(1/(abs(subzipbd@data$zhvi_z)))


nomanh = subset(subzipbd, borough != "Manhattan" )


plot(nomanh@data$frate, nomanh@data$zhvi_z,  pch = 16, ylab = "Z scores for ZHVI", xlab = "frisk rate", col = factor(nomanh@data$borough))
legend("topright", legend = unique(factor(nomanh@data$borough)), col = unique(factor(nomanh@data$borough)), pch = 19)
```



## Conclusions
