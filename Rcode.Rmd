---
title: "Spatial Statistics Final Project"
author: "Brusco, Bianca; Wang, Hongting; Yang, Lingfeng"
date: "9/28/2018"
output: pdf_document
---

    ```{r setup, include = FALSE, warning = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, tidy = TRUE)
    library(lattice)
    library(spdep)
    library(RColorBrewer)
    library(classInt)
    library(rgdal)
    library(formatR)
    library(sm)
    library(tidyverse)
    #library(revgeo)
    library(ggmap)
    library(sp)
    library(reshape2)
    library(lubridate)
    
    #snippet to override r-chunk fonts.
    def.chunk.hook  <- knitr::knit_hooks$get("chunk")
    knitr::knit_hooks$set(chunk = function(x, options) {
       x <- def.chunk.hook(x, options)
       ifelse(options$size != "normalsize", paste0("\\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
    })
    ```

#1. Importing and Cleaning data
##A. Importing data
(i). NYC Stop & Frisk data
(ii). Housing value panel data
```{r init-1,size="tiny",warning=FALSE,message=FALSE}
# Housing value
hvraw <- read.csv("data/Trimmed_home_value.csv", header = T)

# Stop & Frisk
load("data/sqf.RData")
#stops2013 <- subset(stops,year==2013)
#stops2013[1:3,c("date","precinct","suspected.crime","suspect.race","found.weapon")]
```

##B. Cleaning data
###(1). Housing Value
```{r}
# select relevant date range for housing value, filter out other cities
hv <- hvraw %>% filter(City == "New York") %>% select(RegionName, CountyName, X2005.12:X2014.01) %>% arrange(RegionName)

```

###(2). Stop & Frisk

https://www.mapdevelopers.com/geocode_bounding_box.php
For geographical boundaries of NYC.
North Latitude: 40.917577 South Latitude: 40.477399 East Longitude: -73.700272 West Longitude: -74.259090

```{r}
# filter out NAs for lon and lat, 1303 NAs and 47 "1900-12-31" for date, then select columns of interest, sorted in chronological order
sf <- stops %>% filter(lat > 40.2 & !is.na(lat) & lat < 41) %>% filter(date >= "2006-01-01" & date <= "2013-12-31") %>% select(year, date, time, precinct, suspected.crime, frisked, suspect.race, found.weapon, lat, lon) %>% arrange(date, time) %>% mutate(month = substr(date, 6, 7))

```

###(3). shapefiles for zip code bounds and NYC
```{r, echo = F}
# zip code shape file
zipraw <- readOGR("data/zip_code/ZIP_CODE_040114.shp")
nyc <- readOGR("data/Borough_Boundaries/geo_export_5e515234-1937-40b5-b942-1ef10ea3ea45.shp")

# change projection method to lon/lat
zipbd <-  spTransform(zipraw, CRS("+proj=longlat +ellps=WGS84 +no_defs"))

# test
plot(zipbd, lwd = 0.4)
points(sf$lon[1:10000], sf$lat[1:10000], pch = ".", cex = 2, col = sf[1:10000, ]$suspect.race)
```

###(4). Add zip code field to Stop & Frisk based on coordinates
```{r}
# Convert lon/lat into zip code, 300/min -> 18,000/hr -> 432,000/day, 9.15 day LOL
#Sys.time()
#x <- rep(0, 20)
#for (i in 1:20) {
#  x[i] <- as.numeric(revgeo(sf$lon[i], sf$lat[i], output = "hash", item = "zip"))
#}
#Sys.time()

# experimenting with reversing geo code
# update: IT WORKS! 500,000/14s -> takes around 110s to run the whole thing
#Sys.time()
#coord1 <- cbind(sf$lon[1:500000], sf$lat[1:500000])
#points <- SpatialPoints(coord1, CRS("+proj=longlat +ellps=WGS84 +no_defs"))
#y <- over(points, zipbd)
#summary(y)
#Sys.time()
#

coord1 <- cbind(sf$lon, sf$lat)
points <- SpatialPoints(coord1, CRS("+proj=longlat +ellps=WGS84 +no_defs"))
reversezip <- over(points, zipbd)
```
There are 1147 missing values, to further inspect the patterns, the following plot is created

```{r}
plot(zipbd, border = "green", lwd = 0.2)
points(sf$lon[which(is.na(reversezip$ZIPCODE))], sf$lat[which(is.na(reversezip$ZIPCODE))], pch = ".", cex = 2.5, col = "red")
```
It appears that all the missing values lie right on the boundaries. To fix this, we will find 8 surrounding points (all of whom of equal distance to the original point), from north going clockwise. We will then get the zipcode for all those 8 points and exclude NAs before taking a majority vote. Finally, we use the majority vote result to impute the NAs.

**function for matching boundary points**
```{r}
# The distance from the original point is about 370 ft.
zipcorrect <- function(lon, lat, shp) {
  revzip <- rep(NA, length(lon))
  for (i in 1:length(lon)) {
    # starting from north, rotating clockwise for the 8 sample points
    samp.lon <- c(lon[i], lon[i] + 0.001, lon[i] + 0.0014, lon[i] + 0.001, lon[i], lon[i] - 0.001, lon[i] - 0.0014, lon[i] - 0.001)
    samp.lat <- c(lat[i] + 0.0014, lat[i] + 0.001, lat[i], lat[i] - 0.001, lat[i] - 0.0014, lat[i] - 0.001, lat[i], lat[i] + 0.001)
    
    samp.coord <- cbind(samp.lon, samp.lat)
    
    samp.points <- SpatialPoints(samp.coord, CRS("+proj=longlat +ellps=WGS84 +no_defs"))
    
    samp.zip <- as.character(over(samp.points, shp)$ZIPCODE)
    shortzip <- samp.zip[which(!is.na(samp.zip))]
    
    revzip[i] <- shortzip[which.max(tabulate(match(shortzip, unique(shortzip))))]
  }
  return(revzip)
}
```

**Matching boundary points to zipcode**
```{r}
zipmiss <- sf[which(is.na(reversezip$ZIPCODE)), c(10, 9)]
Sys.time()
imputedzip <- zipcorrect(zipmiss$lon, zipmiss$lat, zipbd)
Sys.time()
# 2m32s, 1m58s
```

**Append zipcode and zipcode related fields to Stop & Frisk**
```{r}
sf$zip <- as.character(reversezip$ZIPCODE)
sf$zip[which(is.na(sf$zip))] <- imputedzip
sum(is.na(sf$zip))
```

```{r}
plot(zipbd, border = "red", lwd = 0.2)
points(sf$lon[which(sf$zip == "11232")], sf$lat[which(sf$zip == "11232")], pch = ".", cex = 1, col = sf$year)

plot(zipbd, border = "white", lwd = 0.5, col = zipbd@data$CTY_FIPS)
plot(zipbd, border = "white", lwd = 0.2, col = cm.colors(50, alpha = 0.9)[cut(zipbd@data$POPULATION, 50)])

plot(subset(zipbd, ZIPCODE == "10003"))
points(sf$lon[which(sf$zip == "10003")], sf$lat[which(sf$zip == "10003")], pch = ".", cex = 2, col = sf$year[which(sf$zip == "10003")])

plot(subset(zipbd, COUNTY == "New York"))
points(sf$lon[which(sf$zip == "10003")], sf$lat[which(sf$zip == "10003")], pch = ".", cex = 2, col = sf$year[which(sf$zip == "10003")])
```

##C. group stop and frisk data based zipcode
```{r}
str(zipbd@data)

```

#### NEED TO ADD ALL THE ATTRIBUTES FROM ZIPCODE TO THE DATA
```{r}
jointable <- zipbd@data %>% select(zip = ZIPCODE, po_name = PO_NAME, pop = POPULATION, area = AREA, county = COUNTY) %>% arrange(zip, desc(area))

zipbd@data[246:247, ]

plot(subset(zipbd, ZIPCODE == "10004"))

zipbd@data[c(107, 110, 114, 115), ]
```
Within the jointable, there is one duplicate record of zipcode 10047. There are other regions with multiple polygons associated with one zip code (10004 for instance). For these cases, we aggregate the land area and take any one of the population entry. It is cleaned in the following chunk.

```{r}
jtclean <- jointable[!duplicated(jointable), ]
jtclean <- jtclean %>% group_by(zip) %>% summarize(po_name = first(po_name), pop = first(pop), area = sum(area), county = first(county)) %>% as.data.frame()

jtclean$zip <- as.character(jtclean$zip)
jtclean$county <- as.character(jtclean$county)

jtclean$county[which(jtclean$county == "New York")] <- "Manhattan"
jtclean$county[which(jtclean$county == "Richmond")] <- "Staten Island"
jtclean$county[which(jtclean$county == "Kings")] <- "Brooklyn"
colnames(jtclean)[5] <- "borough"
```


```{r}
sf_large <- left_join(sf, jtclean)

zip_large <- right_join(sf, jtclean)
```

## Group data on zipcode
```{r}

sf_rate <- sf_large %>% group_by(zip, year, month) %>% summarise(frisked = sum(frisked), stops = n(), po_name = first(po_name), pop = first(pop), area = first(area), borough = first(borough)) %>% as.data.frame()

plot(zipbd, border = "white", lwd = 0.2, col = heat.colors(10, alpha = 1)[cut(sf_rate$frisked/sf_rate$stops, 10)])
```

## inspect difference between zipbd, housing value, and stop and frisk
After subsetting zipbd with BLDGZIP == 0 (building zip), it matches perfectly with S&F

However, we are still missing some values for housing price. 


```{r}
nbzipbd <- subset(zipbd, BLDGZIP == 0)

ziphv <- as.character(unique(hv$RegionName))
zipsf <- unique(sf_rate$zip)
zipbound <- as.character(unique(nbzipbd@data$ZIPCODE))
zipdiff1 <- setdiff(zipbound, zipsf)
zipdiff2 <- setdiff(zipbound, ziphv)
zipdiff3 <- setdiff(zipsf, ziphv)

#plot(zipbd, border = "white", lwd = 0.2, col = ifelse(as.character(zipbd@data$ZIPCODE) %in% zipdiff1, "red", "blue"))

plot(nbzipbd, border = "white", lwd = 0.2, col = ifelse(as.character(zipbd@data$ZIPCODE) %in% zipdiff3, "red", "blue"), main = "ZIP codes with missing ZHVI value")

#zipbd@data[which(as.character(zipbd@data$ZIPCODE) %in% zipdiff1), ]


plot(nbzipbd, border = "white", lwd = 0.2, col = "red")
text(coordinates(nbzipbd), labels = zipbd@data$ZIPCODE, cex = 0.6)
```


Now we need to put the ZHVI data in a long format. For each zip, month, year, we want to have a specific ZHVI so we can joint with the rates tabe. 

```{r}
hvlong = melt(hv, id.vars = c("RegionName", "CountyName"), value.name = "zhvi")
hvlong = hvlong[, - 2]  #don't need county name, drop is as it is annoying me

head(hvlong)
hvlong = hvlong %>% mutate(variable = gsub("X","",variable)) %>% mutate(variable = gsub("\\.","-",variable)) %>% arrange(RegionName)
hvlong = hvlong %>%  mutate(variable = as.Date(paste0(variable, "-01"))) %>% mutate(year = lubridate::year(variable), month = lubridate::month(variable)) %>% rename(zip = RegionName)

hvlong = hvlong[,c(1,4,5,3)]
hvlong$zip = as.character(hvlong$zip)

sf_rate$year = as.double(sf_rate$year)
sf_rate$month = as.double(sf_rate$month)

sfhv = left_join(sf_rate, hvlong, by = c("zip","year", "month"))
head(sfhv)

```

## Now narrowing in on housing value and zip code

49 missing, could potentially get some with APT&CONDO from zillow.

```{r}
hvdiffshp <- subset(zipbd, zipbd$ZIPCODE %in% zipdiff2)
plot(nbzipbd, lwd ="0.2")
plot(hvdiffshp, border = "white", lwd = 0.2, col = "red", add = T)
text(coordinates(hvdiffshp), labels = hvdiffshp@data$ZIPCODE, cex = 0.6)
```

## Data cleaning problems to fix:

1. Missing values that can be fixed by adding the values from APP&Condo:

Note: For some areas there is only one time of sale made by Zillow, blablabla explain better later -- bottom line = augment data set with zip_zhvi_condominum



```{r}

hvcraw <- read.csv("data/Zip_Zhvi_Condominum.csv", header = T)

# select relevant date range for housing value, filter out other cities
hvc <- hvcraw %>% filter(City == "New York") %>% select(RegionName, CountyName, X2005.12:X2014.01) %>% arrange(RegionName)


## same cleaning as before.......
hvclong = melt(hvc, id.vars = c("RegionName", "CountyName"), value.name = "zhvi")
hvclong = hvclong[, - 2]  #don't need county name, drop is as it is annoying me


hvclong = hvclong %>% mutate(variable = gsub("X","",variable)) %>% mutate(variable = gsub("\\.","-",variable)) %>% arrange(RegionName)
hvclong = hvclong %>%  mutate(variable = as.Date(paste0(variable, "-01"))) %>% mutate(year = lubridate::year(variable), month = lubridate::month(variable)) %>% rename(zip = RegionName, zhvic = zhvi)

hvclong = hvclong[,c(1,4,5,3)]
hvclong$zip = as.character(hvclong$zip)



sfhvc = left_join(sfhv, hvclong, by = c("zip","year", "month"))
sfhvc
## similar enoguth zhvi to zhvic that we don't care?

##




plot(nbzipbd, border = "white", lwd = 0.2, col = ifelse(as.character(zipbd@data$ZIPCODE) %in% zipdiff3, "red", "blue"), main = "ZIP codes with missing ZHVI value")


plot(nbzipbd, border = "white", lwd = 0.2, col = ifelse(as.character(zipbd@data$ZIPCODE) %in% unique(sfhvc[is.na(sfhvc$zhvi),]$zip), "red", "blue"), main = "ZIP codes with missing ZHVI value")



#sf_rate[sf_rate$zip == "10032" ,]
## only one month available for this zip code
#hvlong[hvlong$zip == 10032,]

#add the values of Apptt$Condo that have value for the zhvi NAs
sfhvc[is.na(sfhvc$zhvi),]$zhvi = sfhvc[is.na(sfhvc$zhvi),]$zhvic

#remove bronx and central park
sfhvc_sub = sfhvc %>% filter(borough != "Bronx", po_name != "Central Park")


plot(nbzipbd, border = "white", lwd = 0.2, col = ifelse(as.character(zipbd@data$ZIPCODE) %in% unique(sfhvc[is.na(sfhvc$zhvi),]$zip), "red", "blue"), main = "ZIP codes with missing ZHVI value")

```


## Kringing

```{r}
#grouping by year
sfhvc_year = sfhvc_sub %>% group_by(zip, year) %>% summarise(frisked = sum(frisked), stops = sum(stops), po_name = first(po_name), pop = mean(pop), area = first(area), borough = first(borough), zhvi = mean(zhvi)) %>% as.data.frame() %>% filter(pop > 0)


#missing......
table(sfhvc_year$year)
length(unique(sfhvc_year$zip))

## don't think this plot is doing what i want it to...
par(mfrow = c(2,4))
for(i in 1:8){
  years = unique(sfhvc_year$year)
  plot(nbzipbd, border = "white", lwd = 0.2, col = ifelse(as.character(zipbd@data$ZIPCODE) %in% unique(sfhvc_year[(sfhvc_year$year == years[i]) & (is.na(sfhvc_year$zhvi)),]$zip), "red", "blue"), main = "")
}

```


## Kringing

total missing values

```{r}
missings = sfhvc_year[is.na(sfhvc_year$zhvi),]
dim(missings) # 166 missing values
unique(missings$zip)

```

---> COMPUTE VARIOGRAM> to do so, we need to have weight matrix. To do so, we need to have the same number of polygons as of zip codes, each polygon associated with it. 





# Doing some testing


```{r}

sfhvc_sub
glm1 = glm(stops ~ zhvi + offset(log(pop)), data = sfhvc_sub, family = poisson)
summary(glm1)

```


